{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ce5b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a5b16",
   "metadata": {},
   "source": [
    "In this implementation we use the California housing dataset to predict housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f1b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 08:31:11.868039: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from bdl.pbp.gamma_initializer import ReciprocalGammaInitializer\n",
    "from bdl.pbp.math import safe_div, non_negative_constraint\n",
    "\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6938cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with California housing dataset\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c1750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_input(x, dtype, input_shape):\n",
    "    x = tf.constant(x, dtype=dtype)\n",
    "    call_rank = tf.rank(tf.constant(0, shape=input_shape, dtype=dtype)) + 1\n",
    "    if tf.rank(x) < call_rank:\n",
    "        x = tf.reshape(x, [-1, * input_shape.as_list()])\n",
    "    return x\n",
    "\n",
    "\n",
    "def ensure_output(y, dtype, output_dim):\n",
    "    output_rank = 2\n",
    "    y = tf.constant(y, dtype=dtype)\n",
    "    if tf.rank(y) < output_rank:\n",
    "        y = tf.reshape(y, [-1, output_dim])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a876842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, y, output_shape):\n",
    "    x = ensure_input(x, tf.float32, x.shape[1])\n",
    "    y = ensure_output(y, tf.float32, output_shape)\n",
    "    mean_X_train, mean_y_train, std_X_train, std_y_train = get_mean_std_x_y(x, y)\n",
    "    x = (x - np.full(x.shape, mean_X_train)) / np.full(x.shape, std_X_train)\n",
    "    y = (y - mean_y_train) / std_y_train\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_mean_std_x_y(x, y):\n",
    "    std_X_train = np.std(x, 0)\n",
    "    std_X_train[std_X_train == 0] = 1\n",
    "    mean_X_train = np.mean(x, 0)\n",
    "    std_y_train = np.std(y)\n",
    "    if std_y_train == 0.0:\n",
    "        std_y_train = 1.0\n",
    "    mean_y_train = np.mean(y)\n",
    "    return mean_X_train, mean_y_train, std_X_train, std_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53b69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = normalize(X_train, y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8962fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBPLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: int, dtype=tf.float32, *args, **kwargs):\n",
    "        super().__init__(dtype=tf.as_dtype(dtype), *args, **kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_shape = tensor_shape.TensorShape(input_shape)\n",
    "        last_dim = tensor_shape.dimension_value(input_shape[-1])\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.inv_sqrtV1 = tf.cast(1.0 / tf.math.sqrt(1.0 * last_dim + 1), dtype=self.dtype)\n",
    "        self.inv_V1 = tf.math.square(self.inv_sqrtV1)\n",
    "\n",
    "        over_gamma = ReciprocalGammaInitializer(6.0, 6.0)\n",
    "        self.kernel_m = self.add_weight(\n",
    "            \"kernel_mean\",\n",
    "            shape=[last_dim, self.units],\n",
    "            initializer=tf.keras.initializers.HeNormal(),\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.kernel_v = self.add_weight(\n",
    "            \"kernel_variance\",\n",
    "            shape=[last_dim, self.units],\n",
    "            initializer=over_gamma,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.bias_m = self.add_weight(\n",
    "            \"bias_mean\",\n",
    "            shape=[self.units],\n",
    "            initializer=tf.keras.initializers.HeNormal(),\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.bias_v = self.add_weight(\n",
    "            \"bias_variance\",\n",
    "            shape=[self.units],\n",
    "            initializer=over_gamma,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.Normal = tfp.distributions.Normal(\n",
    "            loc=tf.constant(0.0, dtype=self.dtype),\n",
    "            scale=tf.constant(1.0, dtype=self.dtype),\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    @tf.function\n",
    "    def apply_gradient(self, gradient):\n",
    "        dlogZ_dkm, dlogZ_dkv, dlogZ_dbm, dlogZ_dbv = gradient\n",
    "\n",
    "        # Kernel\n",
    "        self.kernel_m.assign_add(self.kernel_v * dlogZ_dkm)\n",
    "        new_kv = self.kernel_v - (tf.math.square(self.kernel_v) * (tf.math.square(dlogZ_dkm) - 2 * dlogZ_dkv))\n",
    "        self.kernel_v.assign(non_negative_constraint(new_kv))\n",
    "\n",
    "        # Bias\n",
    "        self.bias_m.assign_add(self.bias_v * dlogZ_dbm)\n",
    "        new_bv = self.bias_v - (tf.math.square(self.bias_v) * (tf.math.square(dlogZ_dbm) - 2 * dlogZ_dbv))\n",
    "        self.bias_v.assign(non_negative_constraint(new_bv))\n",
    "\n",
    "    @tf.function\n",
    "    def _sample_weights(self):\n",
    "        eps_k = self.Normal.sample(self.kernel_m.shape)\n",
    "        std_k = tf.math.sqrt(tf.maximum(self.kernel_v, tf.zeros_like(self.kernel_v)))\n",
    "        W = self.kernel_m + std_k * eps_k\n",
    "\n",
    "        eps_b = self.Normal.sample(self.bias_m.shape)\n",
    "        std_b = tf.math.sqrt(tf.maximum(self.bias_v, tf.zeros_like(self.bias_v)))\n",
    "        b = self.bias_m + std_b * eps_b\n",
    "        return W, b\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x: tf.Tensor):\n",
    "        W, b = self._sample_weights()\n",
    "        return (tf.tensordot(x, W, axes=[1, 0]) + tf.expand_dims(b, axis=0)) * self.inv_sqrtV1\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, previous_mean: tf.Tensor, previous_variance: tf.Tensor):\n",
    "        mean = (\n",
    "            tf.tensordot(previous_mean, self.kernel_m, axes=[1, 0])\n",
    "            + tf.expand_dims(self.bias_m, axis=0)\n",
    "        ) * self.inv_sqrtV1\n",
    "\n",
    "        variance = (\n",
    "            tf.tensordot(previous_variance, tf.math.square(self.kernel_m), axes=[1, 0])\n",
    "            + tf.tensordot(tf.math.square(previous_mean), self.kernel_v, axes=[1, 0])\n",
    "            + tf.expand_dims(self.bias_v, axis=0)\n",
    "            + tf.tensordot(previous_variance, self.kernel_v, axes=[1, 0])\n",
    "        ) * self.inv_V1\n",
    "\n",
    "        return mean, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fc703ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBPReLULayer(PBPLayer):\n",
    "    @tf.function\n",
    "    def call(self, x: tf.Tensor):\n",
    "        \"\"\"Calculate deterministic output\"\"\"\n",
    "        # x is of shape [batch, prev_units]\n",
    "        x = super().call(x)\n",
    "        z = tf.maximum(x, tf.zeros_like(x))  # [batch, units]\n",
    "        return z\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, previous_mean: tf.Tensor, previous_variance: tf.Tensor):\n",
    "        ma, va = super().predict(previous_mean, previous_variance)\n",
    "        mb, vb = get_mb_vb(ma, va, self.Normal)\n",
    "        return mb, vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a43937b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mb_vb(ma, va, normal):\n",
    "    _sqrt_v = tf.math.sqrt(tf.maximum(va, tf.zeros_like(va)))\n",
    "    _alpha = safe_div(ma, _sqrt_v)\n",
    "    _inv_alpha = safe_div(tf.constant(1.0, dtype=_alpha.dtype), _alpha)\n",
    "    _cdf_alpha = normal.cdf(_alpha)\n",
    "    _gamma = tf.where(\n",
    "        _alpha < -30,\n",
    "        -_alpha + _inv_alpha * (-1 + 2 * tf.math.square(_inv_alpha)),\n",
    "        safe_div(normal.prob(-_alpha), _cdf_alpha),\n",
    "    )\n",
    "    _vp = ma + _sqrt_v * _gamma\n",
    "    mb = _cdf_alpha * _vp\n",
    "    vb = mb * _vp * normal.cdf(-_alpha) + _cdf_alpha * va * (\n",
    "            1 - _gamma * (_gamma + _alpha)\n",
    "    )\n",
    "    return mb, vb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3aa835",
   "metadata": {},
   "source": [
    "Our model will be a model with two ReLU layers with two 50 hidden units and a linear last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6cccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [50, 50, 1]\n",
    "\n",
    "def build_layers(last_shape, units):\n",
    "    layers = []\n",
    "    for unit in units[:-1]:\n",
    "        layer = PBPReLULayer(unit)\n",
    "        layer.build(last_shape)\n",
    "        layers.append(layer)\n",
    "        last_shape = unit\n",
    "    layer = PBPLayer(units[-1])\n",
    "    layer.build(last_shape)\n",
    "    layers.append(layer)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c238121",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [50, 50, 1]\n",
    "layers = build_layers(X_train.shape[1], [50, 50, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a31c624",
   "metadata": {},
   "source": [
    "We can now instantiate our PBP model. We see that we define a few more variables that we need for PBP. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a4ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = tf.math.atan(tf.constant(1.0, dtype=tf.float32)) * 4\n",
    "LOG_INV_SQRT2PI = -0.5 * tf.math.log(2.0 * pi)\n",
    "\n",
    "\n",
    "class PBP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: List[tf.keras.layers.Layer],\n",
    "        dtype: Union[tf.dtypes.DType, np.dtype, str] = tf.float32\n",
    "    ):\n",
    "        self.alpha = tf.Variable(6.0, trainable=True, dtype=dtype)\n",
    "        self.beta = tf.Variable(6.0, trainable=True, dtype=dtype)\n",
    "        self.layers = layers\n",
    "        self.Normal = tfp.distributions.Normal(\n",
    "            loc=tf.constant(0.0, dtype=dtype),\n",
    "            scale=tf.constant(1.0, dtype=dtype),\n",
    "        )\n",
    "        self.Gamma = tfp.distributions.Gamma(concentration=self.alpha, rate=self.beta)\n",
    "\n",
    "    def fit(self, x, y, batch_size: int = 16, n_epochs: int = 1):\n",
    "        data = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)\n",
    "        for epoch_index in range(n_epochs):\n",
    "            print(f\"{epoch_index=}\")\n",
    "            for x_batch, y_batch in data:\n",
    "                diff_square, v, v0 = self.update_gradients(x_batch, y_batch)\n",
    "                alpha, beta = update_alpha_beta(self.alpha, self.beta, diff_square, v, v0)\n",
    "                self.alpha.assign(alpha)\n",
    "                self.beta.assign(beta)\n",
    "\n",
    "    @tf.function\n",
    "    def predict(self, x: tf.Tensor):\n",
    "        m, v = x, tf.zeros_like(x)\n",
    "        for layer in self.layers:\n",
    "            m, v = layer.predict(m, v)\n",
    "        return m, v\n",
    "\n",
    "    @tf.function\n",
    "    def update_gradients(self, x, y):\n",
    "        trainables = [layer.trainable_weights for layer in self.layers]\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(trainables)\n",
    "            m, v = self.predict(x)\n",
    "            v0 = v + safe_div(self.beta, self.alpha - 1)\n",
    "            diff_square = tf.math.square(y - m)\n",
    "            logZ0 = logZ(diff_square, v0)\n",
    "        grad = tape.gradient(logZ0, trainables)\n",
    "        for l, g in zip(self.layers, grad):\n",
    "            l.apply_gradient(g)\n",
    "        return diff_square, v, v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c7f60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PBP(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a000ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha_beta(alpha, beta, diff_square, v, v0):\n",
    "    alpha1 = alpha + 1\n",
    "    v1 = v + safe_div(beta, alpha)\n",
    "    v2 = v + beta / alpha1\n",
    "    logZ2_logZ1 = logZ1_minus_logZ2(diff_square, v1=v2, v2=v1)\n",
    "    logZ1_logZ0 = logZ1_minus_logZ2(diff_square, v1=v1, v2=v0)\n",
    "    logZ_diff = logZ2_logZ1 - logZ1_logZ0\n",
    "    Z0Z2_Z1Z1 = safe_exp(logZ_diff)\n",
    "    # Must update beta first\n",
    "    # Extract larger exponential\n",
    "    pos_where = safe_exp(logZ2_logZ1) * (alpha1 - safe_exp(-logZ_diff) * alpha)\n",
    "    neg_where = safe_exp(logZ1_logZ0) * (Z0Z2_Z1Z1 * alpha1 - alpha)\n",
    "    beta_denomi = tf.where(logZ_diff >= 0, pos_where, neg_where)\n",
    "    beta = safe_div(beta, tf.maximum(beta_denomi, tf.zeros_like(beta)))\n",
    "\n",
    "    alpha_denomi = Z0Z2_Z1Z1 * safe_div(alpha1, alpha) - 1.0\n",
    "\n",
    "    alpha = safe_div(\n",
    "        tf.constant(1.0, dtype=alpha_denomi.dtype),\n",
    "        tf.maximum(alpha_denomi, tf.zeros_like(alpha)),\n",
    "    )\n",
    "\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def logZ(diff_square: tf.Tensor, v: tf.Tensor):\n",
    "    v0 = v + 1e-6\n",
    "    return tf.reduce_sum(\n",
    "        -0.5 * (diff_square / v0) + LOG_INV_SQRT2PI - 0.5 * tf.math.log(v0)\n",
    "    )\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def logZ1_minus_logZ2(diff_square: tf.Tensor, v1: tf.Tensor, v2: tf.Tensor):\n",
    "    return tf.reduce_sum(\n",
    "        -0.5 * diff_square * safe_div(v2 - v1, v1 * v2)\n",
    "        - 0.5 * tf.math.log(safe_div(v1, v2) + 1e-6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54c2d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def safe_div(x: tf.Tensor, y: tf.Tensor, eps: tf.Tensor = tf.constant(1e-6)):\n",
    "    _eps = tf.cast(eps, dtype=y.dtype)\n",
    "    return x / (tf.where(y >= 0, y + _eps, y - _eps))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def safe_exp(x: tf.Tensor, BIG: tf.Tensor = tf.constant(20)):\n",
    "    return tf.math.exp(tf.math.minimum(x, tf.cast(BIG, dtype=x.dtype)))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def non_negative_constraint(x: tf.Tensor):\n",
    "    return tf.maximum(x, tf.zeros_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1866e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_index=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-12 08:31:14.032847: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=1, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7b5bf",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea9019fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.651302891855423, -3.2725913393996726, -3.2353817613326985)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalise test set\n",
    "mean_X_train, mean_y_train, std_X_train, std_y_train = get_mean_std_x_y(X_train, y_train)\n",
    "# We normalize the test set\n",
    "X_test = (X_test - np.full(X_test.shape, mean_X_train)) / np.full(X_test.shape, std_X_train)\n",
    "\n",
    "# perform inference on normalised data\n",
    "X_test = ensure_input(X_test, tf.float32, X_test.shape[1])\n",
    "m, v = model.predict(X_test)\n",
    "v_noise = (model.beta / (model.alpha - 1) * std_y_train**2)\n",
    "# add mean and std back to m and v\n",
    "m = m * std_y_train + mean_y_train\n",
    "v = v * std_y_train**2\n",
    "\n",
    "# transform back to original space\n",
    "m = np.squeeze(m.numpy())\n",
    "v = np.squeeze(v.numpy())\n",
    "v_noise = np.squeeze(v_noise.numpy().reshape(-1, 1))\n",
    "\n",
    "# calculate rmse\n",
    "rmse = np.sqrt(np.mean((y_test - m) ** 2))\n",
    "# calculate log-likelihood\n",
    "test_ll = np.mean(\n",
    "    -0.5 * np.log(2 * math.pi * v)\n",
    "    - 0.5 * (y_test - m) ** 2 / v\n",
    ")\n",
    "# calculate log-likelihood with v_noise\n",
    "test_ll_with_vnoise = np.mean(\n",
    "    -0.5 * np.log(2 * math.pi * (v + v_noise))\n",
    "    - 0.5 * (y_test - m) ** 2 / (v + v_noise)\n",
    ")\n",
    "rmse, test_ll, test_ll_with_vnoise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
